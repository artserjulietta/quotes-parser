# Описание JSON файла


Данный код парсит цитаты с сайта https://quotes.toscrape.com/ и генерирует из них JSON файл. Он состоит из списка цитат (100 штук), где каждая цитата является словарем с тремя ключами:
1. "quote" : str (текст цитаты)
2. "author" : dict ```{ "name" : str (имя автора),
     "link" : str (ссылка на автора)}```
4. "tags" : list of dict ```{  "tag" : str (название тега),
      "link" : str (ссылка на тег)}```

Пример словаря с информацией о цитате:
```
{
        "quote": "The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.",
        "author": {
            "name": "Albert Einstein",
            "link": "/author/Albert-Einstein"
        },
        "tags": [
            {
                "tag": "change",
                "link": "/tag/change/page/1/"
            }
        ]
}
```

# Что было сделано 

Была реализована функция **get_all_quotes()**. Функция парсит данные с сайта https://quotes.toscrape.com/ и сохраняет их в JSON-файл "quotes.json".

- Функция проходит по всем 10 страницам https://quotes.toscrape.com/page/ , перебирая номера страниц в цикле.
- На каждой странице функция получает список всех цитат по тегу 'div' и классу 'quote'.
- После чего для каждой цитаты реализован сбор данных: текст цитаты, автор и ссылка на него, список тегов и ссылки на них.
- Каждая цитата представлена в виде словаря, а итоговые данные - это список всех собранных цитат


# Откуда были получены данные

Данные были получены с сайта "https://quotes.toscrape.com/", который содержит коллекцию цитат. Сайт имеет страницы с разными цитатами, каждая из которых содержит текст цитаты, имя автора, ссылку на автора и теги, относящиеся к цитате.


# Как осуществлялся сбор

Для сбора данных потребовалось использование следующие библиотеки:
- **requests**: Библиотека requests была использована для загрузки HTML-кода с сайта. 
- **BeautifulSoup**: Библиотека BeautifulSoup была использована для анализа HTML-кода, чтобы извлечь нужные данные. Она позволяет искать элементы HTML-кода по тегам, классам и другим атрибутам.

В программе был реализован цикл для перебора 10 страниц сайта (с 1-й по 10-ю). Для каждой страницы код выполняет следующие действия:
  - Формирует URL страницы.
  - Отправляет запрос на страницу с помощью requests.get().
  - Парсит HTML-код с помощью BeautifulSoup.
  - Извлекает цитату, автора, ссылку и теги с помощью методов find_all(), get_text() и get('href').
  - Собирает данные о цитате в словарь quote_dict.
  - Добавляет словарь quote_dict в список data.

В конце программы генерируется JSON файл из переменной data и записывается в файл "quotes.json".


# Почему был выбран тот или иной метод/инструмент, а не другой

Данный сайт предоставляет статический HTML-контент, который доступен для просмотра и парсинга с помощью библиотеки BeautifulSoup. На сайте не представлен API, поэтому парсинг данных иным способом был недоступен. 



    
